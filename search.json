[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "colonel_dotto",
    "section": "",
    "text": "This file will become your README and also the index of your documentation."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "colonel_dotto",
    "section": "Install",
    "text": "Install\npip install colonel_dotto"
  },
  {
    "objectID": "dotto.html",
    "href": "dotto.html",
    "title": "dotto",
    "section": "",
    "text": "I was struck by a moment of inspiration at around 1:30am on Tuesday 10/11/22. I want to hark back to something Professor Haddock told me during my exploration of the Colonel Blotto problem during the fall semester of my freshman year, which was to have fun with the research question and ask fundamental questions. These were questions such as given a strategy is there always a minimal change to “improve” a strategy. This can be represented in a directed graph by having the allocations to the castle in a game be represented as a tuple; this tuple will be used as the graph weights for the graphs nodes. Each graph node will represent an individual strategy for the game. I was initially considering the edges to be unweighted because the weighted context did not arise naturally until now. That being, let the weight of an edge from one node to it’s dominating node be \\(\\sum\\limits_{i=1}^{\\text{\\# of castles}}v_i * ?t_i\\), where \\(v_i\\) is the value of the \\(i\\) th castle (which could possibly be weighted by how many extra troops the dominating strategy won by at that castle).\nI returned to this on 11/30/22. Let’s do some damage, using graphs!\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n\ndef preprocess_document(document, sentence_spliter='.', word_spliter=' ', punct_mark=','):\n    # lowercase all words and remove trailing whitespaces\n    document = document.lower().strip()\n    \n    # remove unwanted punctuation marks\n    for pm in punct_mark:\n        document = document.replace(pm, '')\n    \n    # get list of sentences which are non-empty\n    sentences = [sent for sent in document.split(sentence_spliter) if sent != '']\n    \n    # get list of sentences which are lists of words\n    document = []\n    for sent in sentences:\n        words = sent.strip().split(word_spliter)\n        document.append(words)\n        \n    return document\n\ndef get_entities(document):\n    # in our case, entities are all unique words\n    unique_words = []\n    for sent in document:\n        for word in sent:\n            if word not in unique_words:\n                unique_words.append(word)\n    return unique_words\n\ndef get_relations(document):\n    # in our case, relations are bigrams in sentences\n    bigrams = []\n    for sent in document:\n        for i in range(len(sent)-1):\n            # for every word and the next in the sentence\n            pair = [sent[i], sent[i+1]]\n            # only add unique bigrams\n            if pair not in bigrams:\n                bigrams.append(pair)\n    return bigrams\n\ndef build_graph(doc):\n    # preprocess document for standardization\n    pdoc = preprocess_document(doc)\n    \n    # get graph nodes\n    nodes = get_entities(pdoc)\n    \n    # get graph edges\n    edges = get_relations(pdoc)\n    \n    # create graph structure with NetworkX\n    G = nx.Graph()\n    G.add_nodes_from(nodes)\n    G.add_edges_from(edges)\n    \n    return G\n\ndef build_digraph(doc):\n    # preprocess document for standardization\n    pdoc = preprocess_document(doc)\n    \n    # get graph nodes\n    nodes = get_entities(pdoc)\n    \n    # get graph edges\n    edges = get_relations(pdoc)\n    \n    # create graph structure with NetworkX\n    G = nx.DiGraph()\n    G.add_nodes_from(nodes)\n    G.add_edges_from(edges)\n    \n    return G\n\ndef get_weighted_edges(document):\n    # in our case, relations are bigrams in sentences\n    # weights are number of equal bigrams\n    # use a dict to store number of counts\n    bigrams = {}\n    for sent in document:\n        for i in range(len(sent)-1):\n        \n            # transform to hashable key in dict\n            pair = str([sent[i], sent[i+1]])\n            \n            if pair not in bigrams.keys():\n                # weight = 1\n                bigrams[pair] = 1\n            else:\n                # already exists, weight + 1\n                bigrams[pair] += 1\n                \n    # convert to NetworkX standard form each edge connecting nodes u and v = [u, v, weight]\n    weighted_edges_format = []\n    for pair, weight in bigrams.items():\n        # revert back from hashable format\n        w1, w2 = eval(pair)\n        weighted_edges_format.append([w1, w2, weight])\n        \n    return weighted_edges_format\n\ndef build_weighted_digraph(document):\n    # preprocess document for standardization\n    pdoc = preprocess_document(document)\n    \n    # get graph nodes\n    nodes = get_entities(pdoc)\n    \n    # get weighted edges\n    weighted_edges = get_weighted_edges(pdoc)\n    \n    # create graph structure with NetworkX\n    G = nx.DiGraph()\n    G.add_nodes_from(nodes)\n    G.add_weighted_edges_from(weighted_edges)\n    \n    return G\n\ndef plot_graph(G, title=None):\n    # set figure size\n    plt.figure(figsize=(10,10))\n    \n    # define position of nodes in figure\n    pos = nx.nx_agraph.graphviz_layout(G)\n\n    # draw nodes and edges\n    nx.draw_networkx(G, pos=pos, ax=None, with_labels=True)\n    \n    # get edge labels (if any)\n    edge_labels = nx.get_edge_attributes(G, 'weight')\n    \n    # draw edge labels (if any)\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)\n    \n    # plot the title (if any)\n    plt.title(title)\n    \n    plt.show()\n    return\n\n\nBad key \"text.kerning_factor\" on line 4 in\n/gpfs/ysm/project/sumry2022/sumry2022_ta483/conda_envs/dotto/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\nYou probably need to get an updated matplotlibrc file from\nhttp://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\nor from the matplotlib source distribution\n\n\n\nflowers_of_evil = '''Come, lie upon my breast, cruel, insensitive soul,\nAdored tigress, monster with the indolent air\nI want to plunge trembling fingers for a long time\nIn the thickness of your heavy mane,\n\nTo bury my head, full of pain \nIn your skirts redolent of your perfume, \nTo inhale, as from a withered flower, \nThe moldy sweetness of my defunct love.\n\nI wish to sleep. to sleep rather than live. \nIn a slumber doubtful as death, \nI shall remorselessly cover with my kisses \nYour lovely body polished like copper.\n\nTo bury my subdued sobbing \nNothing equals the abyss of your bed, \nPotent oblivion dwells upon your lips \nAnd Lethe flows in your kisses.\n\nMy fate, hereafter my delight, \nI'll obey like one predestined \nDocile martyr, innocent man condemned, \nWhose fervor aggravates the punishment.\n\nI shall suck, to drown my rancor, \nNepenthe and the good hemlock \nFrom the charming tips of those pointed breasts \nThat have never guarded a heart.'''\n\neye_glasses = 'A color in shaving, a saloon is well placed in the centre of an alley.'\n\nexample = \"The person arrived. They sat down.\"\ndocument = eye_glasses\ng = build_graph(document)\ndig = build_digraph(document)\nwdig = build_weighted_digraph(document)\n\n\nplot_graph(g, \"Undirected, unweighted graph\")\nplot_graph(dig, \"Directed, unweighted graph\")\nplot_graph(wdig, \"Directed, weighted graph\")\n\n\n\n\n\n\n\n\n\n\n\n# can be any type of centrality\nnode_scores = nx.betweenness_centrality(g)\n\n# from highest to lowest (hence the negative sign)\nsorted_node_scores = dict(sorted(node_scores.items(), key=lambda item: -item[1]))\nprint(sorted_node_scores)\n# get words corresponding to nodes (in scoring order)\nsorted_nodes = list(sorted_node_scores.keys())\n\n# candidate KW = highest scoring nodes (1/3 of all nodes)\ncandidate_kw = sorted_nodes[:int(len(sorted_nodes)/3)]\n\n{'in': 0.6136363636363636, 'the': 0.48484848484848486, 'centre': 0.4090909090909091, 'of': 0.30303030303030304, 'placed': 0.21212121212121213, 'an': 0.16666666666666669, 'a': 0.1590909090909091, 'well': 0.12121212121212122, 'color': 0.09848484848484848, 'shaving': 0.09848484848484848, 'saloon': 0.06060606060606061, 'is': 0.045454545454545456, 'alley': 0.0}\n\n\n\nimport spacy\nfrom spacy.tokens import Token\nimport nltk\n\ndef transform_uni_to_ngram(unigram_keywords, text):\n    \"\"\"\n  Candidate unigrams that form ngrams in the text\n  are merged and now form a single candidate keyphrase.\n  \"\"\"\n  # set the spacy model\n    nlp = spacy.load(\"en_core_web_sm\")\n\n  # function that tells if spacy token is keyword or not\n    ckw_getter = lambda token: token.text.lower() in unigram_keywords\n  \n  # set that function as token extension (as attribute with ._)\n    Token.set_extension(\"is_ckw\", getter=ckw_getter, force=True)\n\n  # variable to store resulting keyphrases\n    res = []\n  \n  # split original text in sentences\n    sentences = nltk.sent_tokenize(text)\n  \n    for sent in sentences:\n      # start keyphrase as empty string\n      merged_tokens = ''\n      \n      # process sentence with spacy model defined beforehand\n    sent = nlp(sent)\n      \n    for token in sent:\n          # if token is candidate KW (according to func defined before)\n        if token._.is_ckw:\n          \n              # add it to keyphrase\n              merged_tokens += token.text.lower() + ' '\n         \n         # means that sequence of tokens in text that are candidate KW ends\n        else:\n              # check if keyphrase not empty and already not in list of results\n                if merged_tokens != '' and merged_tokens.strip() not in res:\n                  \n                  # only add keyphrases for now, no unigrams\n                    if len(merged_tokens.strip().split(' ')) != 1:\n                        res.append(merged_tokens.strip())\n                  \n                  # set keyphrase as empty string again\n                    merged_tokens = ''\n\n\n    for uni in unigram_keywords:\n          # only add unigram KW to results list if not there already\n        if uni not in res:\n\n            add_uni = True\n\n            for other in res:  \n                  # only add unigram KW to results list if not used in keyphrase somewhere\n                    if uni in other:\n                        add_uni = False\n\n            if add_uni:\n                res.append(uni)   \n  \n    return res\n\n\ntransform_uni_to_ngram(candidate_kw, eye_glasses)\n\n['in the centre of']\n\n\n\nfrom node2vec import Node2Vec\nnode2vec = Node2Vec(g, dimensions=64, walk_length=30, num_walks=200, workers=4)  # Use temp_folder for big graphs\n\n# Embed nodes\nmodel = node2vec.fit(window=10, min_count=1, batch_words=4)  # Any keywords acceptable by gensim.Word2Vec can be passed, `dimensions` and `workers` are automatically passed (from the Node2Vec constructor)\n\n/gpfs/ysm/project/sumry2022/sumry2022_ta483/conda_envs/dotto/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\nComputing transition probabilities: 100%|███████████████████████████| 13/13 [00:00<00:00, 9540.85it/s]\nGenerating walks (CPU: 3): 100%|██████████| 50/50 [00:04<00:00, 12.49it/s]\n\n\n\n# Look for most similar nodes\nmodel.wv.most_similar('centre')\n\n[('of', 0.8744595646858215),\n ('the', 0.7901211380958557),\n ('an', 0.7885516285896301),\n ('alley', 0.7553330063819885),\n ('in', 0.47600799798965454),\n ('color', 0.32520419359207153),\n ('shaving', 0.26742956042289734),\n ('placed', 0.22931842505931854),\n ('well', 0.2011890858411789),\n ('a', 0.1579960584640503)]\n\n\n\nemb=model.wv[[i for i in model.wv.key_to_index]]\nemb.shape\n\n(13, 64)\n\n\n\n\n\n{'in': 0,\n 'a': 1,\n 'an': 2,\n 'of': 3,\n 'centre': 4,\n 'the': 5,\n 'is': 6,\n 'well': 7,\n 'shaving': 8,\n 'placed': 9,\n 'color': 10,\n 'saloon': 11,\n 'alley': 12}\n\n\n\nfrom sklearn.decomposition import PCA\ndef plot_embeddings(emb, num_nodes, title=\"\"):\n    if emb.shape[1] > 2:\n        pca = PCA(n_components=2)\n        emb= pca.fit_transform(emb)\n    plt.figure()\n    plt.figure()\n    sc = plt.scatter(emb[:,0],emb[:,1])\n    for i, label in enumerate(sorted_nodes):\n        plt.annotate(label, (emb[:,0][i], emb[:,1][i]))\n    plt.suptitle(title)\n    plt.show()\n\n\nplot_embeddings(emb, len(sorted_node_scores))\n\n<Figure size 640x480 with 0 Axes>\n\n\n\n\n\nColonel Blotto below.\n\nimport csv\nimport itertools\n\ndef sums(length, total_sum):\n    if length == 1:\n        yield (total_sum,)\n    else:\n        for value in range(total_sum + 1):\n            for permutation in sums(length - 1, total_sum - value):\n                yield (value,) + permutation\n\ndef game(player1, player2, numCastles):\n    p1_score = 0\n    p2_score = 0\n    n = 0\n    p1_wbt = 0\n    p2_wbt = 0\n\n    while p1_wbt < 3 and p2_wbt < 3 and n < numCastles:\n        if player1[n] > player2[n]:\n            p1_score += n + 1\n            p1_wbt += 1\n            p2_wbt = 0\n        if player1[n] < player2[n]:\n            p2_score += n + 1\n            p2_wbt += 1\n            p1_wbt = 0\n        if player1[n] == player2[n]:\n            p1_wbt = 0\n            p2_wbt = 0\n        n += 1\n\n    while p1_wbt == 3 and n < numCastles:\n        p1_score += n + 1\n        n += 1\n    while p2_wbt == 3 and n < numCastles:\n        p2_score += n + 1\n        n += 1\n\n    if p1_score > p2_score:\n        return 1\n    if p1_score < p2_score:\n        return -1\n    if p1_score == p2_score:\n        return 0\n    \ndef tournament(numTroops, numCastles):\n    strategies = list(sums(numCastles, numTroops))\n    competitors = {}\n    for allocation in strategies:\n        competitors[allocation] = [0, 0, 0]\n    file_data = []\n    highest_score = 0\n    gameMatrix = [[0 for i in range(len(strategies))] for j in range(len(strategies))]\n    for i, player1 in enumerate(competitors):\n        for j, player2 in enumerate(competitors):\n            result = game(player1, player2, numCastles)\n            gameMatrix[i][j] = result\n            if result == 1:\n                competitors[player1][0] += 1\n                competitors[player2][1] += 1\n            if result == -1:\n                competitors[player2][0] += 1\n                competitors[player1][1] += 1\n            else:\n                competitors[player1][2] += 1\n                competitors[player2][2] += 1\n    file_data.append(competitors)\n    file_data.append(gameMatrix)\n    filename = f\"C{numCastles:02}-T{numTroops:03}.txt\"\n\n    with open(filename, \"w\") as fle:\n        json.dump(file_data, fle)\n\ndef csvtournament(numTroops, numCastles):\n    strategies = list(sums(numCastles, numTroops))\n    competitors = {}\n    for allocation in strategies:\n        competitors[allocation] = [0, 0, 0]\n    winners = []\n    highest_score = 0\n    gameMatrix = [[0 for i in range(len(strategies))] for j in range(len(strategies))] \n    for i, player1 in enumerate(competitors):\n        for j, player2 in enumerate(competitors):\n            result = game(player1, player2, numCastles)\n            gameMatrix[i][j] = result\n            if result == 1:\n                competitors[player1][0] += 1 # Player 1 win\n                competitors[player2][1] += 1 # Player 2 loss\n            if result == -1:\n                competitors[player2][0] += 1 # Player 2 win\n                competitors[player1][1] += 1 # Player 1 loss\n            if result == 0:\n                competitors[player1][2] += 1 # tie\n                competitors[player1][2] += 1 # tie\n    all_scores = competitors.values()\n    highest_score = max(all_scores)\n    for player in competitors:\n        if competitors[player] == highest_score:\n            winners.append(player)\n    print(gameMatrix)\n\n    fields = [\"win_percentile\"]\n    for i in range(1, numCastles + 1):\n        fields.append(\"c\" + str(i))\n    rows = []\n    for strategy in competitors:\n        profile = [(competitors[strategy][0]/competitors[strategy][0] + competitors[strategy][1] + competitors[strategy][2])]\n        for troops in strategy:\n            profile.append(troops)\n        rows.append(profile)\n    filename = 'csvBlotto, numCastles = ' + str(numCastles) + \", numTroops =\" + str(numTroops)\n\n# writing to csv file\n    with open(filename, 'w') as csvfile:\n    # creating a csv writer object\n        csvwriter = csv.writer(csvfile)\n\n    # writing the fields\n        csvwriter.writerow(fields)\n\n    # writing the data rows\n        csvwriter.writerows(rows)\n\ndef dottocsv(numTroops, numCastles):\n    strategies = list(sums(numCastles, numTroops))\n    edgelist = []\n\n    unique = list(itertools.combinations(range(len(strategies)), r=2))\n    \n    for pair in unique:\n        i = pair[0]\n        j = pair[1]\n        player1 = strategies[i]\n        player2 = strategies[j]\n    \n        result = game(player1, player2, numCastles)\n\n        if result == 1:\n        # Player 1 win, Player 2 loss\n            edge = (j, i)\n            edgelist.append(edge)\n        if result == -1:\n        # Player 2 win, Player 1 loss\n            edge = (i, j)\n            edgelist.append(edge)\n        #if result == 0:\n        # Tie\n \n\n    filename = f'T{numTroops}C{numCastles}'\n\n    f = open(filename, 'w')\n    f.write(str(edgelist))\n    f.close()\n\n\ndottocsv(2, 3)\n\n\nimport ast\n\nf = open(\"./T2C3\", 'r')\nedgelist = ast.literal_eval(f.readline())\n\n\nGraphtype = nx.DiGraph() \nGraph =  nx.from_edgelist(edgelist, create_using=Graphtype)\nplot_graph(Graph)\nA = nx.adjacency_matrix(Graph)\nprint(A.todense())\n\n\n\n\n[[0 1 0 0 0 0]\n [0 0 0 0 0 0]\n [1 1 0 1 0 0]\n [1 1 0 0 0 0]\n [1 1 1 1 0 1]\n [1 0 1 1 0 0]]"
  }
]